<!DOCTYPE html>

<html lang="en" data-content_root="./">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Reports &#8212; simplebench 0.1.0alpha0 documentation</title>
    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=c309799c" />
    <link rel="stylesheet" type="text/css" href="_static/classic.css?v=39f38206" />
    
    <script src="_static/documentation_options.js?v=9a12e387"></script>
    <script src="_static/doctools.js?v=9bcbadda"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="prev" title="Command-Line Options" href="command_line_options.html" /> 
  </head><body>
    <div class="related" role="navigation" aria-label="Related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="right" >
          <a href="command_line_options.html" title="Command-Line Options"
             accesskey="P">previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="index.html">simplebench 0.1.0alpha0 documentation</a> &#187;</li>
        <li class="nav-item nav-item-this"><a href="">Reports</a></li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <section id="reports">
<h1>Reports<a class="headerlink" href="#reports" title="Link to this heading">ğŸ”—</a></h1>
<div class="toctree-wrapper compound" id="simplebench-reports">
</div>
<section id="rich-table-report">
<h2>Rich Table Report<a class="headerlink" href="#rich-table-report" title="Link to this heading">ğŸ”—</a></h2>
<div class="toctree-wrapper compound" id="id1">
</div>
<p>Rich Tables are a popular way to display benchmark results in a clear and
concise manner. SimpleBench leverages the <a class="reference external" href="https://github.com/Textualize/rich">rich</a>
library to generate these tables, providing visually appealing and easy-to-read reports.</p>
<p>To generate a Rich Table report, you can use the <cite>â€“rich-table.ops</cite> command-line
option when running your benchmarks. This will produce tables that summarize the
performance metrics for each benchmark, including operations per second, average
time per operation, memory usage, and peak memory usage.</p>
<p>Here is an example of how to run a benchmark script with Rich Table reporting:</p>
<div class="literal-block-wrapper docutils container" id="run-benchmark-rich-table">
<div class="code-block-caption"><span class="caption-text">Running a benchmark with Rich Table report</span><a class="headerlink" href="#run-benchmark-rich-table" title="Link to this code">ğŸ”—</a></div>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="w">  </span>python<span class="w"> </span>my_benchmark_script.py<span class="w"> </span>--rich-table.ops<span class="w"> </span>--progress
</pre></div>
</div>
</div>
<p>This command will execute the benchmarks defined in <cite>my_benchmark_script.py</cite> and
generate a Rich Table report displaying the operations-per-second results in
the terminal. A progress bar will also be shown during the execution of the benchmarks.</p>
<p>A basic Rich Table output will look something like this:</p>
<div class="literal-block-wrapper docutils container" id="basic-sample-rich-table-output">
<div class="code-block-caption"><span class="caption-text">Basic Sample Rich Table Output</span><a class="headerlink" href="#basic-sample-rich-table-output" title="Link to this code">ğŸ”—</a></div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>                                                               addition_benchmark
                                                              operations per second

                                          A simple addition benchmark of Python&#39;s built-in sum function.
  â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
  â”ƒ        â”ƒ            â”ƒ        â”ƒ Elapsed â”ƒ    mean    â”ƒ   median   â”ƒ           â”ƒ            â”ƒ            â”ƒ             â”ƒ  std dev   â”ƒ        â”ƒ
  â”ƒ   N    â”ƒ Iterations â”ƒ Rounds â”ƒ Seconds â”ƒ   kOps/s   â”ƒ   kOps/s   â”ƒ min Ops/s â”ƒ max kOps/s â”ƒ 5th kOps/s â”ƒ 95th kOps/s â”ƒ   kOps/s   â”ƒ  rsd%  â”ƒ
  â”¡â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
  â”‚      1 â”‚    46701   â”‚      1 â”‚  0.32   â”‚    148.00  â”‚    149.00  â”‚   876.00  â”‚    153.00  â”‚    143.00  â”‚    151.00   â”‚      8.99  â”‚  6.08% â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</pre></div>
</div>
</div>
<p>The table includes various statistics such as the number of iterations, rounds,
elapsed time, mean and median operations per second, minimum and maximum operations
per second, as well as standard deviation and relative standard deviation (rsd%).</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>To avoid â€œfalse precisionâ€, statistical results are shown to three significant digits.
Due to the inherent variability of performance measurement, any further digits are
typically meaningless statistical noise.</p>
<p>This is not an issue with SimpleBench itself, but rather a fundamental aspect of benchmarking and performance
measurement in the real world.</p>
</div>
<p>The basic fields always displayed in a Rich Table report are:</p>
<dl>
<dt>N</dt><dd><p>A complexity weighting used to indicate the input size for a benchmark.</p>
<p>A Big-O (<em>O</em>(<em>n</em>), etc) complexity weighting. This is used to indicate the â€˜sizeâ€™ of the
input to a parameterized benchmark. It defaults to 1 unless overridden by the benchmark.
The N value is used to help compare performance across different input sizes (if applicable)
and to analyze how the function scales with different input sizes.</p>
</dd>
<dt>Iterations</dt><dd><p>The number of statistical samples taken for the benchmark.</p>
<p>The total number of iterations executed during the benchmark. An iteration is an execution of the benchmarked
function once for statistical reporting purposes. It may be composed of multiple actual rounds to improve accuracy
and precision, but is reported as a single count for the purposes of the table.</p>
</dd>
<dt>Rounds</dt><dd><p>The number of times the benchmarked function is executed within a single iteration.</p>
<p>The number of rounds executed during an iteration. A round is a single execution of the benchmarked function.
Multiple rounds are often executed within an iteration to gather more accurate timing and performance data.
They are executed in rapid succession, and their results are aggregated to produce the final metrics for an iteration.</p>
</dd>
<dt>Elapsed Seconds</dt><dd><p>The total CPU time spent executing the benchmarked code.</p>
<p>The total measured elapsed time in seconds for all iterations of the benchmark. This metric provides an overview
of how long the benchmark took to complete. This does not include any setup or teardown time, focusing solely
on the execution time of the benchmarked code. By default, this measures <em>CPU time</em>, not <em>wall-clock time</em>, to provide
a more accurate representation of the codeâ€™s performance. It can be overridden to measure wall-clock time instead
if so desired.</p>
</dd>
<dt>mean Ops/s</dt><dd><p>The average number of operations per second.</p>
<p>The arithmetic mean average number of of operations per second (Ops/s) performed during the benchmark.
This metric is calculated by dividing the total number of operations executed by the total elapsed time,
then scaling it an appropriate factor (for example, kOps/s) for easier readability. It provides a quick overview
of the benchmarkâ€™s performance.</p>
</dd>
<dt>median Ops/s</dt><dd><p>The 50th percentile (middle value) of operations per second.</p>
<p>The median (50th percentile) number of operations per second (Ops/s) performed during the benchmark.
This metric represents the middle value of the Ops/s measurements collected during the benchmark,
providing a robust measure of central tendency that is less affected by outliers compared to the mean.</p>
</dd>
<dt>min Ops/s</dt><dd><p>The lowest (worst) performance recorded across all iterations.</p>
<p>The minimum number of operations per second (Ops/s) recorded during the benchmark. This metric indicates the
lowest performance observed during the benchmark runs, which can be useful for identifying potential bottlenecks
or performance issues.</p>
</dd>
<dt>max Ops/s</dt><dd><p>The highest (best) performance recorded across all iterations.</p>
<p>The maximum number of operations per second (Ops/s) recorded during the benchmark. This metric indicates the
highest performance observed during the benchmark runs, showcasing the best-case scenario for the benchmarked code.</p>
</dd>
<dt>5th Ops/s</dt><dd><p>The 5th percentile of operations per second.</p>
<p>The 5th percentile number of operations per second (Ops/s) recorded during the benchmark. This metric indicates
that 5% of the Ops/s measurements were below this value, providing insight into the lower end of the typical
performance distribution.</p>
</dd>
<dt>95th Ops/s</dt><dd><p>The 95th percentile of operations per second.</p>
<p>The 95th percentile number of operations per second (Ops/s) recorded during the benchmark. This metric indicates
that 95% of the Ops/s measurements were below this value, providing insight into the upper end of the typical
performance distribution.</p>
</dd>
<dt>std dev kOps/s</dt><dd><p>A measure of the variation or inconsistency in performance.</p>
<p>The standard deviation of the operations per second (Ops/s) measurements collected during the benchmark. This metric
quantifies the amount of variation or dispersion in the Ops/s values, providing insight into the consistency of the
benchmarkâ€™s performance. A lower standard deviation indicates more consistent performance, while a higher standard
deviation suggests greater variability in the results.</p>
</dd>
<dt>rsd%</dt><dd><p>A normalized measure of performance inconsistency, expressed as a percentage.</p>
<p>The relative standard deviation (RSD) expressed as a percentage. This metric is calculated by dividing the standard
deviation by the mean and multiplying by 100. It provides a normalized measure of variability, allowing for easier
comparison of consistency across different benchmarks or configurations. A lower RSD% indicates more consistent
performance relative to the mean, while a higher RSD% suggests greater variability in the results.</p>
</dd>
</dl>
</section>
</section>


            <div class="clearer"></div>
          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="Main">
        <div class="sphinxsidebarwrapper">
  <div>
    <h3><a href="index.html">Table of Contents</a></h3>
    <ul>
<li><a class="reference internal" href="#">Reports</a><ul>
<li><a class="reference internal" href="#rich-table-report">Rich Table Report</a></li>
</ul>
</li>
</ul>

  </div>
  <div>
    <h4>Previous topic</h4>
    <p class="topless"><a href="command_line_options.html"
                          title="previous chapter">Command-Line Options</a></p>
  </div>
  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="_sources/reports.rst.txt"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div>
<search id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</search>
<script>document.getElementById('searchbox').style.display = "block"</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="Related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="right" >
          <a href="command_line_options.html" title="Command-Line Options"
             >previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="index.html">simplebench 0.1.0alpha0 documentation</a> &#187;</li>
        <li class="nav-item nav-item-this"><a href="">Reports</a></li> 
      </ul>
    </div>
    <div class="footer" role="contentinfo">
    &#169; Copyright 2025, Jerilyn Franz.
      Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 8.1.3.
    </div>
  </body>
</html>